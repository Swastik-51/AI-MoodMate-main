{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e6d154",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Fundamentals\n",
    "\n",
    "This notebook contains comprehensive examples of Natural Language Processing techniques using Python. The examples cover text preprocessing, feature extraction, classification, and advanced NLP algorithms.\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install scikit-learn spacy gensim pandas numpy\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "\n",
    "## Table of Contents\n",
    "1. [Text Preprocessing and Feature Extraction](#preprocessing)\n",
    "2. [Classification and Machine Learning](#classification)\n",
    "3. [Advanced NLP Techniques](#advanced)\n",
    "4. [Text Analysis and Search](#analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bda4fc",
   "metadata": {},
   "source": [
    "## 1. Text Preprocessing and Feature Extraction <a id=\"preprocessing\"></a>\n",
    "\n",
    "### Text Preprocessing Pipeline\n",
    "Clean and normalize raw text data for NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9592b4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['email', 'like', 'filter', 'love', 'nlp', 'visit']\n"
     ]
    }
   ],
   "source": [
    "# file: 1_preprocess.py\n",
    "import re\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # small, fast English model\n",
    "\n",
    "def basic_clean(text: str) -> str:\n",
    "    # lower, strip urls/emails/@mentions/hashtags, keep letters/numbers/space/apostrophe\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"(http\\S+|www\\.\\S+)\", \" \", text)\n",
    "    text = re.sub(r\"\\S+@\\S+\", \" \", text)\n",
    "    text = re.sub(r\"[@#]\\w+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z0-9\\s']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize_stop_lemma(text: str) -> List[str]:\n",
    "    doc = nlp(text)\n",
    "    out = []\n",
    "    for tok in doc:\n",
    "        if tok.is_space or tok.is_punct:\n",
    "            continue\n",
    "        lemma = tok.lemma_.lower().strip()\n",
    "        if len(lemma) < 3:           # drop very short tokens\n",
    "            continue\n",
    "        if lemma in ENGLISH_STOP_WORDS:  # sklearn's built-in stoplist\n",
    "            continue\n",
    "        out.append(lemma)\n",
    "    return out\n",
    "\n",
    "def preprocess(text: str) -> List[str]:\n",
    "    return tokenize_stop_lemma(basic_clean(text))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    s = \"Emails like help@site.com are filtered. I'm LOVING NLP!!! Visit https://x.y.\"\n",
    "    print(preprocess(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69eb219",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) Representation\n",
    "Convert text documents to numerical feature vectors using word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d9490e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['and' 'are' 'coding' 'fun' 'in' 'is' 'language' 'love' 'natural' 'nlp'\n",
      " 'powerful' 'processing' 'python']\n",
      "\n",
      "Bag of Words Matrix:\n",
      "   and  are  coding  fun  in  is  language  love  natural  nlp  powerful  \\\n",
      "0    0    0       0    0   0   0         1     1        1    0         0   \n",
      "1    0    0       0    1   0   1         1     0        0    0         0   \n",
      "2    0    0       1    0   1   0         0     1        0    0         0   \n",
      "3    1    1       0    0   0   0         0     0        0    1         1   \n",
      "\n",
      "   processing  python  \n",
      "0           1       0  \n",
      "1           1       0  \n",
      "2           0       1  \n",
      "3           0       1  \n",
      " ['and' 'are' 'coding' 'fun' 'in' 'is' 'language' 'love' 'natural' 'nlp'\n",
      " 'powerful' 'processing' 'python']\n",
      "\n",
      "Bag of Words Matrix:\n",
      "   and  are  coding  fun  in  is  language  love  natural  nlp  powerful  \\\n",
      "0    0    0       0    0   0   0         1     1        1    0         0   \n",
      "1    0    0       0    1   0   1         1     0        0    0         0   \n",
      "2    0    0       1    0   1   0         0     1        0    0         0   \n",
      "3    1    1       0    0   0   0         0     0        0    1         1   \n",
      "\n",
      "   processing  python  \n",
      "0           1       0  \n",
      "1           1       0  \n",
      "2           0       1  \n",
      "3           0       1  \n"
     ]
    }
   ],
   "source": [
    "# file: 3_bow_demo.py\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Language processing is fun\",\n",
    "    \"I love coding in Python\",\n",
    "    \"Python and NLP are powerful\"\n",
    "]\n",
    "\n",
    "# Create Bag of Words model\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to dataframe for readability\n",
    "bow_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"\\nBag of Words Matrix:\")\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef473751",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization\n",
    "Weight words by their importance using Term Frequency-Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c03eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['advances' 'and' 'artificial' 'deep' 'fun' 'intelligence' 'is' 'learning'\n",
      " 'machine']\n",
      "\n",
      "TF-IDF Matrix:\n",
      "   advances    and  artificial   deep    fun  intelligence     is  learning  \\\n",
      "0     0.000  0.000       0.000  0.000  0.609          0.00  0.609     0.360   \n",
      "1     0.552  0.000       0.000  0.552  0.000          0.42  0.000     0.326   \n",
      "2     0.000  0.552       0.552  0.000  0.000          0.42  0.000     0.326   \n",
      "\n",
      "   machine  \n",
      "0    0.360  \n",
      "1    0.326  \n",
      "2    0.326  \n"
     ]
    }
   ],
   "source": [
    "# file: 4_tfidf_demo.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"machine learning is fun\",\n",
    "    \"deep learning advances machine intelligence\",\n",
    "    \"artificial intelligence and machine learning\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF model\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(docs)\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(X.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691371da",
   "metadata": {},
   "source": [
    "### Word2Vec Embeddings\n",
    "Create dense vector representations of words that capture semantic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68cdcda2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# file: 5_word2vec_demo.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Sample sentences (tokenized)\u001b[39;00m\n\u001b[32m      5\u001b[39m sentences = [\n\u001b[32m      6\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mi\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlove\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnatural\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprocessing\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      7\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mlanguage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprocessing\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfun\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      8\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mdeep\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlearning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33madvances\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33martificial\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mintelligence\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      9\u001b[39m     [\u001b[33m\"\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mis\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgreat\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfor\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmachine\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlearning\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m ]\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "# file: 5_word2vec_demo.py\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample sentences (tokenized)\n",
    "sentences = [\n",
    "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"language\", \"processing\", \"is\", \"fun\"],\n",
    "    [\"deep\", \"learning\", \"advances\", \"artificial\", \"intelligence\"],\n",
    "    [\"python\", \"is\", \"great\", \"for\", \"machine\", \"learning\"]\n",
    "]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=2, sg=1)\n",
    "\n",
    "# Explore embeddings\n",
    "print(\"Vector for 'language':\\n\", model.wv[\"language\"])\n",
    "print(\"\\nMost similar to 'learning':\", model.wv.most_similar(\"learning\"))\n",
    "print(\"\\nSimilarity between 'python' and 'language':\", model.wv.similarity(\"python\", \"language\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34201fff",
   "metadata": {},
   "source": [
    "## 2. Classification and Machine Learning <a id=\"classification\"></a>\n",
    "\n",
    "### Text Classification with TF-IDF and Logistic Regression\n",
    "Perform sentiment analysis using machine learning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 2_classify_tfidf.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# tiny demo dataset (positive/negative sentiment)\n",
    "texts = [\n",
    "    \"I loved this movie, fantastic acting and great story\",\n",
    "    \"This film was terrible and boring\",\n",
    "    \"Absolutely wonderful experience, highly recommend\",\n",
    "    \"Worst acting ever, do not watch\",\n",
    "    \"It was okay, some parts were fun\",\n",
    "    \"I hated the plot, very disappointing\",\n",
    "    \"Brilliant direction and superb cast\",\n",
    "    \"Not good, waste of time\",\n",
    "    \"Enjoyable and engaging from start to finish\",\n",
    "    \"Awful soundtrack and weak story\"\n",
    "]\n",
    "labels = np.array([1,0,1,0,1,0,1,0,1,0])  # 1=pos, 0=neg\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(ngram_range=(1,2), min_df=1)),\n",
    "    (\"clf\",   LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# try the model\n",
    "samples = [\"pretty good but slow in places\", \"utterly awful, I want my time back\"]\n",
    "print(\"Predictions:\", pipe.predict(samples))\n",
    "print(\"Class probabilities:\", pipe.predict_proba(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9880fc",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning with Grid Search\n",
    "Optimize model performance by systematically testing different parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce73fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 3_tune_grid.py\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"excellent movie with great acting\",\n",
    "    \"terrible plot and awful pacing\",\n",
    "    \"loved every moment, fantastic!\",\n",
    "    \"boring and predictable\",\n",
    "    \"superb cinematography and direction\",\n",
    "    \"weak script and bad acting\",\n",
    "    \"what a masterpiece\",\n",
    "    \"not good at all\",\n",
    "    \"brilliant experience overall\",\n",
    "    \"do not recommend\"\n",
    "]\n",
    "y = np.array([1,0,1,0,1,0,1,0,1,0])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"clf\", LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1),(1,2)],\n",
    "    \"tfidf__min_df\": [1,2],\n",
    "    \"tfidf__analyzer\": [\"word\", \"char_wb\"],\n",
    "    \"clf__C\": [0.25, 1.0, 4.0]  # regularization strength\n",
    "}\n",
    "\n",
    "search = GridSearchCV(pipe, param_grid, cv=3, n_jobs=-1, scoring=\"f1\")\n",
    "search.fit(texts, y)\n",
    "\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV score (f1):\", search.best_score_)\n",
    "best_model = search.best_estimator_\n",
    "print(\"Sample prediction:\", best_model.predict([\"not a great movie but had moments\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce513346",
   "metadata": {},
   "source": [
    "### Naive Bayes Classification\n",
    "Simple probabilistic classifier that works well with text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d059c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 6_nb_classify.py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Tiny sentiment dataset\n",
    "texts = [\n",
    "    \"I love this movie\",\n",
    "    \"This film was awful\",\n",
    "    \"Amazing performance and great story\",\n",
    "    \"Boring and too long\",\n",
    "    \"Fantastic acting\",\n",
    "    \"Terrible direction\"\n",
    "]\n",
    "labels = [1, 0, 1, 0, 1, 0]  # 1=positive, 0=negative\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Bag of Words + Naive Bayes\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_bow)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca380cd",
   "metadata": {},
   "source": [
    "## 3. Advanced NLP Techniques <a id=\"advanced\"></a>\n",
    "\n",
    "### Topic Modeling with Latent Dirichlet Allocation (LDA)\n",
    "Discover hidden topics in document collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caef7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 4_topic_lda.py\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "docs = [\n",
    "    \"cats purr and sleep on the sofa\", \n",
    "    \"dogs bark and love to play fetch\",\n",
    "    \"kittens and puppies are adorable\",\n",
    "    \"stocks rallied as the market rose\",\n",
    "    \"investors expect inflation to ease\",\n",
    "    \"central bank raised interest rates\"\n",
    "]\n",
    "\n",
    "# bag-of-words for LDA\n",
    "cv = CountVectorizer(stop_words=\"english\")\n",
    "X = cv.fit_transform(docs)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "words = cv.get_feature_names_out()\n",
    "\n",
    "def show_topics(model, feature_names, topn=6):\n",
    "    for i, comp in enumerate(model.components_):\n",
    "        terms = comp.argsort()[::-1][:topn]\n",
    "        print(f\"Topic {i}:\",\" \".join(feature_names[t] for t in terms))\n",
    "\n",
    "show_topics(lda, words)\n",
    "\n",
    "# infer topics for a new doc\n",
    "import numpy as np\n",
    "new = cv.transform([\"the puppy sleeps while the dog plays\"])\n",
    "print(\"Topic distribution:\", np.round(lda.transform(new), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1799fb70",
   "metadata": {},
   "source": [
    "### Topic Modeling with Gensim\n",
    "Alternative implementation using Gensim library for more robust topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435aebbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 8_topic_modeling.py\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Example dataset\n",
    "docs = [\n",
    "    \"I love deep learning and natural language processing\",\n",
    "    \"Artificial intelligence is the future\",\n",
    "    \"Cooking and baking are my hobbies\",\n",
    "    \"I enjoy trying new recipes in the kitchen\",\n",
    "    \"Machine learning and AI are closely related\"\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "texts = [doc.lower().split() for doc in docs]\n",
    "\n",
    "# Create dictionary & corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Train LDA model (2 topics)\n",
    "lda_model = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Show topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde32b0",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER) and Part-of-Speech Tagging\n",
    "Extract entities and analyze grammatical structure using spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba541b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 5_spacy_ner_pos.py\n",
    "import spacy\n",
    "from pprint import pprint\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = (\"Apple is opening a new office in Bengaluru next quarter. \"\n",
    "        \"Tim Cook met Karnataka officials on September 3, 2025 to discuss expansion.\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"\\nNamed Entities (text, label):\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:<25}  -> {ent.label_}\")\n",
    "\n",
    "print(\"\\nPart-of-Speech & Lemmas:\")\n",
    "for token in doc:\n",
    "    if not token.is_space:\n",
    "        print(f\"{token.text:<15} POS={token.pos_:<5}  Lemma={token.lemma_}\")\n",
    "\n",
    "print(\"\\nNoun chunks (base NPs):\")\n",
    "pprint([chunk.text for chunk in doc.noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5ec850",
   "metadata": {},
   "source": [
    "## 4. Text Analysis and Search <a id=\"analysis\"></a>\n",
    "\n",
    "### Semantic Search\n",
    "Find relevant documents using cosine similarity and TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ac29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 6_semantic_search.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "corpus = [\n",
    "    \"deep learning methods for image classification\",\n",
    "    \"convolutional neural networks for vision\",\n",
    "    \"natural language processing with transformers\",\n",
    "    \"classical machine learning with SVM and logistic regression\",\n",
    "    \"transfer learning for NLP tasks\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "def search(query: str, topk=3):\n",
    "    q = vectorizer.transform([query])\n",
    "    sims = cosine_similarity(q, X).ravel()\n",
    "    top_idx = sims.argsort()[::-1][:topk]\n",
    "    results = [(corpus[i], float(sims[i])) for i in top_idx]\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for item, score in search(\"best models for text classification\", topk=3):\n",
    "        print(f\"{score:.3f}  {item}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80faf77d",
   "metadata": {},
   "source": [
    "### Document Similarity Analysis\n",
    "Compute similarity between documents using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bbe9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 7_similarity_demo.py\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example documents\n",
    "docs = [\n",
    "    \"I love machine learning and NLP\",\n",
    "    \"NLP and machine learning are amazing\",\n",
    "    \"Cooking recipes are fun to try\",\n",
    "]\n",
    "\n",
    "# TF-IDF representation\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Compute cosine similarity\n",
    "sim_matrix = cosine_similarity(X)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\\n\", sim_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6834c",
   "metadata": {},
   "source": [
    "### Extractive Text Summarization\n",
    "Automatically summarize text by selecting the most important sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d91f783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file: 7_summarize_extractive.py\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "def sent_split(text: str):\n",
    "    # lightweight splitter; for production use nltk or spacy\n",
    "    sents = re.split(r'(?<=[.!?])\\s+', text.strip())\n",
    "    return [s for s in sents if s]\n",
    "\n",
    "def summarize(text: str, max_sentences=3):\n",
    "    sents = sent_split(text)\n",
    "    words = re.findall(r\"[a-zA-Z']+\", text.lower())\n",
    "    words = [w for w in words if w not in ENGLISH_STOP_WORDS and len(w) > 2]\n",
    "    freqs = Counter(words)\n",
    "    # normalize frequencies\n",
    "    if not freqs: return \" \".join(sents[:max_sentences])\n",
    "    maxf = max(freqs.values())\n",
    "    for w in freqs: freqs[w] /= maxf\n",
    "\n",
    "    # sentence scores = sum of word scores\n",
    "    scored = []\n",
    "    for i, s in enumerate(sents):\n",
    "        ws = re.findall(r\"[a-zA-Z']+\", s.lower())\n",
    "        score = sum(freqs.get(w, 0.0) for w in ws) / (len(ws) + 1e-9)\n",
    "        scored.append((score, i, s))\n",
    "\n",
    "    # keep top sentences in original order\n",
    "    top = sorted(sorted(scored, key=lambda x: -x[0])[:max_sentences], key=lambda x: x[1])\n",
    "    return \" \".join(s for _, _, s in top)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = (\n",
    "        \"Transformers have revolutionized natural language processing. \"\n",
    "        \"By leveraging self-attention, they capture long-range dependencies effectively. \"\n",
    "        \"Pretraining on large corpora enables strong performance on many tasks. \"\n",
    "        \"However, transformers can be computationally expensive. \"\n",
    "        \"Researchers explore efficient architectures and distillation to reduce cost.\"\n",
    "    )\n",
    "    print(summarize(text, max_sentences=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf4ae8c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered fundamental Natural Language Processing techniques:\n",
    "\n",
    "### Key Concepts Covered:\n",
    "1. **Text Preprocessing**: Cleaning, tokenization, lemmatization, stop word removal\n",
    "2. **Feature Extraction**: Bag of Words, TF-IDF, Word2Vec embeddings\n",
    "3. **Classification**: Logistic Regression, Naive Bayes, hyperparameter tuning\n",
    "4. **Advanced NLP**: Topic modeling (LDA), Named Entity Recognition, POS tagging\n",
    "5. **Text Analysis**: Semantic search, document similarity, text summarization\n",
    "\n",
    "### Libraries and Tools:\n",
    "- **spaCy**: Industrial-strength NLP with pre-trained models\n",
    "- **scikit-learn**: Machine learning algorithms and text vectorization\n",
    "- **Gensim**: Topic modeling and word embeddings\n",
    "- **Regular Expressions**: Text cleaning and pattern matching\n",
    "\n",
    "### Best Practices:\n",
    "1. Always preprocess text before applying ML algorithms\n",
    "2. Use appropriate feature extraction methods for your task\n",
    "3. Evaluate models with proper train/test splits\n",
    "4. Consider using pre-trained models for better performance\n",
    "5. Handle class imbalance in classification tasks\n",
    "\n",
    "### Next Steps:\n",
    "- Explore transformer models (BERT, GPT) with Hugging Face\n",
    "- Learn about neural language models and deep learning for NLP\n",
    "- Practice with larger, real-world datasets\n",
    "- Experiment with multilingual NLP models\n",
    "- Study recent advances like attention mechanisms and transfer learning\n",
    "\n",
    "### Applications:\n",
    "- **Sentiment Analysis**: Customer feedback, social media monitoring\n",
    "- **Document Classification**: Email filtering, content categorization\n",
    "- **Information Extraction**: Named entity recognition, relation extraction\n",
    "- **Search and Recommendation**: Semantic search, content recommendation\n",
    "- **Text Generation**: Chatbots, automated writing assistance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
