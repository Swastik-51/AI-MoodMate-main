{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2022cba",
   "metadata": {},
   "source": [
    "# NumPy Fundamentals and Machine Learning Basics\n",
    "\n",
    "This notebook contains comprehensive examples of NumPy operations and machine learning fundamentals using scikit-learn. The examples progress from basic array operations to advanced machine learning algorithms.\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install numpy scikit-learn matplotlib pandas\n",
    "```\n",
    "\n",
    "## Table of Contents\n",
    "1. [NumPy Fundamentals](#numpy)\n",
    "2. [Machine Learning Basics](#ml-basics)\n",
    "3. [Advanced Machine Learning](#ml-advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3269c97",
   "metadata": {},
   "source": [
    "## 1. NumPy Fundamentals <a id=\"numpy\"></a>\n",
    "\n",
    "### Array Creation Methods\n",
    "Learn different ways to create NumPy arrays for various use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ccda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creating a 1D array\n",
    "arr1 = np.array([1, 2, 3, 4, 5])\n",
    "print(\"1D Array:\", arr1)\n",
    "\n",
    "# Creating a 2D array (matrix)\n",
    "arr2 = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"2D Array:\\n\", arr2)\n",
    "\n",
    "# Creating an array of zeros\n",
    "zeros = np.zeros((3, 3))\n",
    "print(\"3x3 Zero Matrix:\\n\", zeros)\n",
    "\n",
    "# Creating an array of ones\n",
    "ones = np.ones((2, 4))\n",
    "print(\"2x4 Ones Matrix:\\n\", ones)\n",
    "\n",
    "# Creating an array with a range of numbers\n",
    "range_arr = np.arange(0, 10, 2)  # from 0 to 10 with step 2\n",
    "print(\"Range Array:\", range_arr)\n",
    "\n",
    "# Creating an array with equally spaced values\n",
    "lin_arr = np.linspace(0, 1, 5)  # 5 values between 0 and 1\n",
    "print(\"Linearly spaced values:\", lin_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a0796",
   "metadata": {},
   "source": [
    "### Array Operations and Vectorization\n",
    "Perform element-wise mathematical operations efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf5b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([10, 20, 30, 40])\n",
    "b = np.array([1, 2, 3, 4])\n",
    "\n",
    "# Element-wise addition\n",
    "print(\"Addition:\", a + b)\n",
    "\n",
    "# Element-wise subtraction\n",
    "print(\"Subtraction:\", a - b)\n",
    "\n",
    "# Element-wise multiplication\n",
    "print(\"Multiplication:\", a * b)\n",
    "\n",
    "# Element-wise division\n",
    "print(\"Division:\", a / b)\n",
    "\n",
    "# Square root\n",
    "print(\"Square root of a:\", np.sqrt(a))\n",
    "\n",
    "# Exponentiation\n",
    "print(\"a squared:\", np.power(a, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02044a7f",
   "metadata": {},
   "source": [
    "### Array Indexing and Slicing\n",
    "Access and modify array elements using various indexing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a46dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([10, 20, 30, 40, 50, 60])\n",
    "\n",
    "# Accessing elements\n",
    "print(\"First element:\", arr[0])\n",
    "print(\"Last element:\", arr[-1])\n",
    "\n",
    "# Slicing\n",
    "print(\"First 3 elements:\", arr[0:3])\n",
    "print(\"Every second element:\", arr[::2])\n",
    " \n",
    "# Modifying elements\n",
    "arr[2] = 99\n",
    "print(\"Modified array:\", arr)\n",
    "\n",
    "# 2D array slicing\n",
    "mat = np.array([[1, 2, 3],\n",
    "                [4, 5, 6],\n",
    "                [7, 8, 9]])\n",
    "\n",
    "print(\"Element at (1,2):\", mat[1, 2])   # row 1, col 2\n",
    "print(\"First row:\", mat[0, :])\n",
    "print(\"Second column:\", mat[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add48697",
   "metadata": {},
   "source": [
    "### Statistical Functions\n",
    "Compute descriptive statistics and aggregate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654314af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.array([3, 7, 2, 9, 5])\n",
    "\n",
    "print(\"Max:\", np.max(arr))\n",
    "print(\"Min:\", np.min(arr))\n",
    "print(\"Sum:\", np.sum(arr))\n",
    "print(\"Mean:\", np.mean(arr))\n",
    "print(\"Standard Deviation:\", np.std(arr))\n",
    "print(\"Index of Max Value:\", np.argmax(arr))\n",
    "print(\"Index of Min Value:\", np.argmin(arr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c977fe7",
   "metadata": {},
   "source": [
    "### Random Number Generation\n",
    "Generate random data for testing, simulation, and data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b8300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Random integers between 1 and 10 (size=5)\n",
    "rand_ints = np.random.randint(1, 10, size=5)\n",
    "print(\"Random Integers:\", rand_ints)\n",
    "\n",
    "# Random floats between 0 and 1\n",
    "rand_floats = np.random.rand(5)\n",
    "print(\"Random Floats:\", rand_floats)\n",
    "\n",
    "# Random 3x3 matrix \n",
    "rand_matrix = np.random.randn(3, 3)\n",
    "print(\"Random Normal Distribution Matrix:\\n\", rand_matrix)\n",
    "\n",
    "# Shuffling\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "np.random.shuffle(arr)\n",
    "print(\"Shuffled Array:\", arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce721773",
   "metadata": {},
   "source": [
    "## 2. Machine Learning Basics <a id=\"ml-basics\"></a>\n",
    "\n",
    "### Train-Test Split\n",
    "Properly divide datasets for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1a887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7], [8]])  # Features\n",
    "y = np.array([2, 4, 6, 8, 10, 12, 14, 16])              # Labels\n",
    "\n",
    "# Splitting the dataset (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training Data:\\n\", X_train, y_train)\n",
    "print(\"Testing Data:\\n\", X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5400d2f6",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "Predict continuous values using linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Training data\n",
    "X = np.array([[1], [2], [3], [4], [5]])  # Feature\n",
    "y = np.array([2, 4, 6, 8, 10])           # Label (y = 2x)\n",
    "\n",
    "# Model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "pred = model.predict([[6]])\n",
    "print(\"Prediction for 6:\", pred)\n",
    "\n",
    "# Model parameters\n",
    "print(\"Slope (Coefficient):\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ceca86",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Binary classification using logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c477866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Simple dataset: study hours â†’ pass/fail\n",
    "X = np.array([[1], [2], [3], [4], [5], [6], [7]])\n",
    "y = np.array([0, 0, 0, 1, 1, 1, 1])  # 0 = Fail, 1 = Pass\n",
    "\n",
    "# Model\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "print(\"Prediction for 2.5 hours:\", model.predict([[2.5]]))\n",
    "print(\"Prediction for 6 hours:\", model.predict([[6]]))\n",
    "\n",
    "# Probabilities\n",
    "print(\"Probabilities for 2.5 hours:\", model.predict_proba([[2.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fee8f3",
   "metadata": {},
   "source": [
    "### Decision Tree Classification\n",
    "Tree-based classification with interpretable decision rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba54ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train Decision Tree\n",
    "clf = DecisionTreeClassifier(max_depth=3)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Prediction\n",
    "print(\"Prediction for first sample:\", clf.predict([X[0]]))\n",
    "\n",
    "# Visualize tree\n",
    "plt.figure(figsize=(10,6))\n",
    "tree.plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3199bd07",
   "metadata": {},
   "source": [
    "### Feature Scaling and Standardization\n",
    "Normalize features to improve algorithm performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779d714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Example dataset\n",
    "X = np.array([[10, 100],\n",
    "              [20, 200],\n",
    "              [30, 300],\n",
    "              [40, 400]])\n",
    "\n",
    "# Standardization (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "print(\"Original Data:\\n\", X)\n",
    "print(\"Standardized Data:\\n\", X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ee586",
   "metadata": {},
   "source": [
    "## 3. Advanced Machine Learning <a id=\"ml-advanced\"></a>\n",
    "\n",
    "### K-Means Clustering\n",
    "Unsupervised learning to group similar data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345815d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Sample dataset (2D points)\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# Create KMeans model (2 clusters)\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Cluster centers and labels\n",
    "print(\"Cluster Centers:\\n\", kmeans.cluster_centers_)\n",
    "print(\"Labels:\", kmeans.labels_)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X[:, 0], X[:, 1], c=kmeans.labels_, cmap='viridis')\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "            color='red', marker='X', s=200, label=\"Centroids\")\n",
    "plt.legend()\n",
    "plt.title(\"KMeans Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af2a4c",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "Dimensionality reduction while preserving most of the data variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e16007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Original Shape:\", X.shape)\n",
    "print(\"Reduced Shape:\", X_pca.shape)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, cmap=\"viridis\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"PCA on Iris Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d17bf",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering\n",
    "Alternative clustering approach that builds a tree of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bdcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1, 2], [2, 3], [3, 4],\n",
    "              [8, 7], [9, 6], [10, 8]])\n",
    "\n",
    "# Hierarchical Clustering (2 clusters)\n",
    "clustering = AgglomerativeClustering(n_clusters=2)\n",
    "labels = clustering.fit_predict(X)\n",
    "\n",
    "print(\"Cluster Labels:\", labels)\n",
    "\n",
    "# Visualization\n",
    "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='rainbow')\n",
    "plt.title(\"Agglomerative (Hierarchical) Clustering\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1da32",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook covered fundamental NumPy operations and machine learning concepts:\n",
    "\n",
    "### NumPy Fundamentals:\n",
    "1. **Array Creation**: Various methods to create arrays (zeros, ones, ranges, linear spaces)\n",
    "2. **Operations**: Element-wise arithmetic, vectorization benefits\n",
    "3. **Indexing**: Accessing and modifying array elements\n",
    "4. **Statistics**: Descriptive statistics and aggregate functions\n",
    "5. **Random**: Random number generation for testing and simulation\n",
    "\n",
    "### Machine Learning Basics:\n",
    "1. **Data Preparation**: Train-test splitting, feature scaling\n",
    "2. **Supervised Learning**: Linear regression, logistic regression, decision trees\n",
    "3. **Unsupervised Learning**: K-means clustering, PCA, hierarchical clustering\n",
    "4. **Model Evaluation**: Predictions, probabilities, visualization\n",
    "\n",
    "### Key Libraries:\n",
    "- **NumPy**: Foundation for numerical computing in Python\n",
    "- **scikit-learn**: Comprehensive machine learning library\n",
    "- **Matplotlib**: Visualization and plotting\n",
    "\n",
    "### Best Practices:\n",
    "1. Always use train-test splits for proper evaluation\n",
    "2. Scale features when using distance-based algorithms\n",
    "3. Visualize data and results when possible\n",
    "4. Use random seeds for reproducible results\n",
    "5. Start with simple models before trying complex ones\n",
    "\n",
    "### Algorithm Selection Guide:\n",
    "- **Linear Regression**: Continuous target, linear relationship\n",
    "- **Logistic Regression**: Binary classification, interpretable\n",
    "- **Decision Trees**: Non-linear relationships, interpretable rules\n",
    "- **K-Means**: Spherical clusters, known number of clusters\n",
    "- **PCA**: Dimensionality reduction, visualization\n",
    "- **Hierarchical Clustering**: Unknown number of clusters, dendrograms\n",
    "\n",
    "### Next Steps:\n",
    "- Explore ensemble methods (Random Forest, Gradient Boosting)\n",
    "- Learn neural networks and deep learning (TensorFlow, PyTorch)\n",
    "- Study cross-validation and model selection techniques\n",
    "- Practice with real-world datasets (Kaggle competitions)\n",
    "- Understand bias-variance tradeoff and overfitting\n",
    "\n",
    "### Applications:\n",
    "- **Regression**: Price prediction, sales forecasting\n",
    "- **Classification**: Image recognition, spam detection\n",
    "- **Clustering**: Customer segmentation, market research\n",
    "- **Dimensionality Reduction**: Data visualization, feature selection\n",
    "- **Preprocessing**: Data cleaning, feature engineering"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
